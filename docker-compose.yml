version: '3'

services:
  # --- 1. STORAGE LAYER (HDFS) ---
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    env_file:
      - ./hadoop.env
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - namenode_data:/hadoop/dfs/name

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    environment:
      - SERVICE_PRECONDITION=namenode:9000
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    volumes:
      - datanode_data:/hadoop/dfs/data

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    restart: always
    volumes:
      - ./hadoop_data/datanode2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9000"
    env_file:
      - ./hadoop.env

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode3
    restart: always
    volumes:
      - ./hadoop_data/datanode3:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9000"
    env_file:
      - ./hadoop.env

  # --- 2. OPERATIONAL LAYER (PostgreSQL) ---
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: procurement_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # --- 3. METADATA LAYER ---
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    user: "root"
    ports:
      - "9083:9083"
    environment:
      - SERVICE_NAME=metastore
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=jdbc:derby:/metastore_storage/metastore_db;create=true
    volumes:
      - hive_data:/user/hive/warehouse
      - metastore_db_data:/metastore_storage
    entrypoint: /bin/sh
    command: >
      -c "
        export HIVE_CONF_DIR=/opt/hive/conf;
        if [ ! -f /metastore_storage/.schema_initialized ]; then
          echo 'Initializing schema for the first time...';
          /opt/hive/bin/schematool -dbType derby -initSchema -verbose &&
          touch /metastore_storage/.schema_initialized;
        else
          echo 'Schema already initialized, skipping...';
        fi;
        echo 'Removing stale Derby lock files...';
        rm -f /metastore_storage/metastore_db/*.lck /metastore_storage/metastore_db/db.lck /metastore_storage/metastore_db/dbex.lck;
        echo 'Starting Hive Metastore...';
        /opt/hive/bin/hive --service metastore
      "

  # --- 4. COMPUTE LAYER (Trino) ---
  trino:
    image: trinodb/trino:latest
    container_name: trino
    environment:
      - HADOOP_CONF_DIR=/etc/hadoop/conf
    ports:
      - "8080:8080"
    volumes:
      - ./trino-etc:/etc/trino
      - ./hadoop-conf:/etc/hadoop/conf
    depends_on:
      - namenode
      - postgres
      - hive-metastore

  # --- 5. ORCHESTRATION LAYER (Airflow) ---
  airflow:
    build:
      context: .
      dockerfile: Dockerfile        # ‚Üê uses your custom image with dependencies installed
    container_name: airflow
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    volumes:
      - ./dags:/opt/airflow/dags
      - ./generated_data:/opt/airflow/generated_data
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8081:8080"
    user: "0:0"
    depends_on:
      - postgres
      - namenode
      - trino
      - hive-metastore
    command: airflow standalone

volumes:
  namenode_data:
  datanode_data:
  postgres_data:
  hive_data:
  metastore_db_data: